# Title: Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks
+ Referrer: HJ, Level: 10
+ Reasons: propose a mechanism to solve the problem: once the previous prediction is wrong, the following predictions are influenced in a bad way.
+ [PDF Link](https://arxiv.org/pdf/1506.03099.pdf)  
+ Organization: Google
+ Ref: Samy Bengio, Oriol Vinyals, Navdeep Jaitly, Noam Shazeer: Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks. NIPS 2015: 1171-1179

# Contributions
The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead.

# Model Structure
![Scheduled Sampling](../Images/Scheduled_Sampling.png)
